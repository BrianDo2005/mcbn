{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as DF\n",
    "\n",
    "from mcbn.utils.helper import get_setup\n",
    "from mcbn.utils.helper import dump_yaml\n",
    "from mcbn.utils.helper import get_directories_in_path\n",
    "from mcbn.utils.helper import get_logger\n",
    "from mcbn.environment.constants import HYPERPARAMS_EVAL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "\n",
    "logger.info(\"STEP 3: Getting best hyperparameter choices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Keep track of parsed datasets\n",
    "parsed_datasets = []\n",
    "\n",
    "# Store all results of parsed datasets in a dataframe\n",
    "results_df = None\n",
    "\n",
    "# Get evaluations performed in order from last evaluation to first\n",
    "evaluation_dirs = sorted(os.listdir(HYPERPARAMS_EVAL_PATH), reverse=True)\n",
    "\n",
    "# Iterate over all evaluation dirs\n",
    "for eval_dir in evaluation_dirs:\n",
    "    \n",
    "    # Get all dataset-specific subdirs in evaluation dir\n",
    "    eval_path = os.path.join(HYPERPARAMS_EVAL_PATH, eval_dir)\n",
    "    dataset_dirs = get_directories_in_path(eval_path)\n",
    "    \n",
    "    # Iterate over dataset-specific subdirs\n",
    "    for dataset_name in dataset_dirs:\n",
    "        \n",
    "        # Make sure we have not added a later evaluation of this dataset to results\n",
    "        if not dataset_name in parsed_datasets:\n",
    "            \n",
    "            dataset_eval_path = os.path.join(eval_path, dataset_name)\n",
    "            results_file_path = os.path.join(dataset_eval_path, 'dataset-results.csv')\n",
    "            \n",
    "            # Check that a results file exists (i.e. we are not currently running this evaluation)\n",
    "            if os.path.exists(results_file_path):\n",
    "\n",
    "                # Load results dataframe\n",
    "                df = DF.from_csv(results_file_path)\n",
    "                \n",
    "                # For each result, add its corresponding test index\n",
    "                df['original_index'] = df.index\n",
    "                for model_name, group in df.groupby('model'):\n",
    "                    model_df = group.reset_index(drop=True)\n",
    "                    df.loc[model_df['original_index'], 'test_index'] = model_df.index\n",
    "                df.drop('original_index', axis=1, inplace=True)\n",
    "                \n",
    "                # For each result, add the path to the trained models for all folds\n",
    "                def get_relative_path_of_trained_models(results_row):\n",
    "                    \n",
    "                    # Get test dir name\n",
    "                    test_dir_name = 'test_{}'.format(int(results_row.test_index))\n",
    "                    \n",
    "                    # Get base model name\n",
    "                    model_name = results_row.model\n",
    "                    base_model_name = model_name.replace('MC', '')\n",
    "                    \n",
    "                    # Return relative path\n",
    "                    abs_path = os.path.join(dataset_eval_path, base_model_name, test_dir_name, model_name)\n",
    "                    return os.path.relpath(abs_path, os.getcwd())\n",
    "                \n",
    "                df['path'] = df.apply(get_relative_path_of_trained_models, axis=1)\n",
    "                \n",
    "                # Mark dataset as added\n",
    "                parsed_datasets.append(dataset_name)\n",
    "                \n",
    "                # Append df to collection of all results \n",
    "                results_df = df.reset_index(drop=True) if results_df is None else results_df.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logger.info(results_df.groupby(['dataset_name', 'model', 'batch_size']).cv_rmse.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = results_df.groupby(['dataset_name', 'model']).cv_rmse.transform(min) == results_df.cv_rmse\n",
    "best_results_df = results_df[idx]\n",
    "logger.info(best_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summarize best results in a dict to be dumped as yml\n",
    "parsed_datasets = list(set(d for d in best_results_df.dataset_name)) \n",
    "parsed_models = list(set(d for d in best_results_df.model))\n",
    "\n",
    "best_results_dict = {d: {m: {} for m in parsed_models} for d in parsed_datasets}\n",
    "\n",
    "for i, row in best_results_df.iterrows():\n",
    "    config_dict_keys = ['batch_size', 'lambda', 'dropout', 'cv_rmse', 'path', 'cv_epoch']\n",
    "    best_results_dict[row.dataset_name][row.model] = { k: row[k] for k in config_dict_keys if not pd.isnull(row[k])}\n",
    "\n",
    "dump_yaml(best_results_dict, os.getcwd(), 'grid_search_results.yml')\n",
    "logger.info(\"DONE STEP 3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
