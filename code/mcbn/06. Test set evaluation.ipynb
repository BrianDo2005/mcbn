{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as DF\n",
    "\n",
    "import mcbn.data.dataset_loaders as dl\n",
    "from mcbn.data.dataset import Dataset\n",
    "\n",
    "from mcbn.models.model_bn import ModelBN\n",
    "from mcbn.models.model_do import ModelDO\n",
    "\n",
    "from mcbn.utils.metrics import rmse, pll, crps, pll_maximum, crps_minimum\n",
    "from mcbn.utils.helper import get_setup\n",
    "from mcbn.utils.helper import get_grid_search_results\n",
    "from mcbn.utils.helper import get_tau_results\n",
    "from mcbn.utils.helper import dump_yaml\n",
    "from mcbn.utils.helper import get_new_dir_in_parent_path\n",
    "from mcbn.utils.helper import make_path_if_missing\n",
    "from mcbn.utils.helper import get_train_and_evaluation_models\n",
    "from mcbn.utils.helper import get_logger\n",
    "\n",
    "from mcbn.environment.constants import TEST_EVAL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "\n",
    "logger.info(\"STEP 6: Test set evaluation\")\n",
    "\n",
    "RANDOM_SEED_NP_FIRST_RUN = 1\n",
    "RANDOM_SEED_TF_FIRST_RUN = 1\n",
    "\n",
    "# Read in config\n",
    "s = get_setup()\n",
    "\n",
    "# Read in grid search results\n",
    "g = get_grid_search_results()\n",
    "\n",
    "# Read in tau optimization results\n",
    "t = get_tau_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluate_dataset(c, X_train, y_train, X_test, y_test, tf_seed):\n",
    "    \n",
    "    dataset = Dataset(X_train, \n",
    "                      y_train, \n",
    "                      X_test, \n",
    "                      y_test, \n",
    "                      s['discard_leftovers'],\n",
    "                      normalize_X=s['normalize_X'], \n",
    "                      normalize_y=s['normalize_y'])\n",
    "    \n",
    "    # Initialize results dict\n",
    "    results = {k: [] for k in ['model', 'epoch', 'PLL', 'CRPS', 'RMSE']}\n",
    "    \n",
    "    # Get and initialize model\n",
    "    with tf.Graph().as_default() as g:\n",
    "\n",
    "        with g.device('/cpu:0'):\n",
    "            \n",
    "            # Set random generator seed for reproducible models\n",
    "            tf.set_random_seed(tf_seed)\n",
    "            \n",
    "            # Note: Tau must be set to base model (MCBN or MCDO) tau for get_mc_moments \n",
    "            # to be able to find var (overridden for const in metrics calc)\n",
    "            if c['base_model_name'] in ['BN', 'MCBN']:\n",
    "                model = ModelBN(s['n_hidden'],\n",
    "                                K=c['k'],\n",
    "                                nonlinearity=s['nonlinearity'],\n",
    "                                bn=True,\n",
    "                                do=False,\n",
    "                                tau=c['taus'][c['base_model_name']],\n",
    "                                dataset=dataset,\n",
    "                                in_dim=c['in_dim'],\n",
    "                                out_dim=c['out_dim'])\n",
    "            elif c['base_model_name'] in ['DO', 'MCDO']:\n",
    "                keep_prob = 1 - c['dropout']\n",
    "                model = ModelDO(s['n_hidden'], \n",
    "                                K=c['k'], \n",
    "                                nonlinearity=s['nonlinearity'], \n",
    "                                bn=False, \n",
    "                                do=True,\n",
    "                                tau=c['taus'][c['base_model_name']], \n",
    "                                dataset=dataset, \n",
    "                                in_dim=c['in_dim'], \n",
    "                                out_dim=c['out_dim'],\n",
    "                                first_layer_do=True)\n",
    "\n",
    "            model.initialize(l2_lambda=c['lambda'], learning_rate=s['learning_rate'])\n",
    "\n",
    "        # Start session (regular session is default session in with statement)\n",
    "        with tf.Session(config=tf.ConfigProto(\n",
    "                allow_soft_placement=False,\n",
    "                log_device_placement=False,\n",
    "                inter_op_parallelism_threads=1,\n",
    "                intra_op_parallelism_threads=1)) as sess:\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # Train model n_epochs iterations and get test results\n",
    "            last_epoch = 1\n",
    "            while dataset.curr_epoch <= c['n_epochs']:\n",
    "                \n",
    "                new_epoch = last_epoch < dataset.curr_epoch\n",
    "                last_epoch = dataset.curr_epoch\n",
    "                \n",
    "                # Each eval_interval:th epoch, save all metrics\n",
    "                if new_epoch and (dataset.curr_epoch % s['test_eval_interval'] == 0\n",
    "                                  or dataset.curr_epoch == c['n_epochs']):\n",
    "                    \n",
    "                    # BN BASED MODELS\n",
    "                    if c['base_model_name'] == 'MCBN':\n",
    "                        # MCBN\n",
    "                        samples = model.get_mc_samples(s['mc_samples'], dataset.X_test, c['batch_size'])\n",
    "                        mean, var = model.get_mc_moments(samples)\n",
    "                        \n",
    "                        results['model'] += ['MCBN']\n",
    "                        results['epoch'] += [dataset.curr_epoch]\n",
    "                        results['PLL']   += [pll(samples, dataset.y_test, s['mc_samples'], c['taus']['MCBN'])]\n",
    "                        results['RMSE']  += [rmse(mean, dataset.y_test)]\n",
    "                        results['CRPS']  += [crps(dataset.y_test, mean, var)]\n",
    "\n",
    "                        # MCBN const\n",
    "                        results['model'] += ['MCBN const']\n",
    "                        results['epoch'] += [dataset.curr_epoch]\n",
    "                        results['PLL']   += [pll(np.array([mean]), dataset.y_test, 1, c['taus']['MCBN const'])]\n",
    "                        results['RMSE']  += [rmse(mean, dataset.y_test)]\n",
    "                        results['CRPS']  += [crps(dataset.y_test, mean, c['taus']['MCBN const']**(-1))]\n",
    "                        \n",
    "                        # At final epoch, save prediction mean, var and true y\n",
    "                        if dataset.curr_epoch == c['n_epochs']:\n",
    "                            final_predictions = {\n",
    "                                'yHat': mean.ravel(),\n",
    "                                'MCBN var': var.ravel(),\n",
    "                                'MCBN const var': [c['taus']['MCBN const']**(-1)] * len(mean),\n",
    "                                'y': dataset.y_test.ravel()\n",
    "                            }\n",
    "                            optimum_predictions = {\n",
    "                                'MCBN PLL_opt': [pll_maximum(mean, dataset.y_test)],\n",
    "                                'MCBN CRPS_opt': [crps_minimum(mean, dataset.y_test)]\n",
    "                            }\n",
    "                    \n",
    "                    elif c['base_model_name'] == 'BN':\n",
    "                        model.update_layer_statistics(dataset.X_train)\n",
    "                        samples = model.predict(dataset.X_test)\n",
    "                        \n",
    "                        results['model'] += ['BN']\n",
    "                        results['epoch'] += [dataset.curr_epoch]\n",
    "                        results['PLL']   += [pll(np.array([samples]), dataset.y_test, 1, model.tau)]\n",
    "                        results['RMSE']  += [rmse(samples, dataset.y_test)]\n",
    "                        results['CRPS']  += [crps(dataset.y_test, samples, model.tau**(-1))]\n",
    "                    \n",
    "                    # DO BASED MODELS\n",
    "                    elif c['base_model_name'] == 'MCDO':\n",
    "                        # MCDO\n",
    "                        samples = model.get_mc_samples(s['mc_samples'], dataset.X_test, keep_prob)\n",
    "                        mean, var = model.get_mc_moments(samples)\n",
    "\n",
    "                        results['model'] += ['MCDO']\n",
    "                        results['epoch'] += [dataset.curr_epoch]\n",
    "                        results['PLL']   += [pll(samples, dataset.y_test, s['mc_samples'], c['taus']['MCDO'])]\n",
    "                        results['RMSE']  += [rmse(mean, dataset.y_test)]\n",
    "                        results['CRPS']  += [crps(dataset.y_test, mean, var)]\n",
    "\n",
    "                        # MCDO const\n",
    "                        results['model'] += ['MCDO const']\n",
    "                        results['epoch'] += [dataset.curr_epoch]\n",
    "                        results['PLL']   += [pll(np.array([mean]), dataset.y_test, 1, c['taus']['MCDO const'])]\n",
    "                        results['RMSE']  += [rmse(mean, dataset.y_test)]\n",
    "                        results['CRPS']  += [crps(dataset.y_test, mean, c['taus']['MCDO const']**(-1))]\n",
    "                        \n",
    "                        # At final epoch, save prediction mean, var and true y\n",
    "                        if dataset.curr_epoch == c['n_epochs']:\n",
    "                            final_predictions = {\n",
    "                                'yHat': mean.ravel(),\n",
    "                                'MCDO var': var.ravel(),\n",
    "                                'MCDO const var': [c['taus']['MCDO const']**(-1)] * len(mean),\n",
    "                                'y': dataset.y_test.ravel()\n",
    "                            }\n",
    "                            optimum_predictions = {\n",
    "                                'MCDO PLL_opt': [pll_maximum(mean, dataset.y_test)],\n",
    "                                'MCDO CRPS_opt': [crps_minimum(mean, dataset.y_test)]\n",
    "                            }\n",
    "                        \n",
    "                    elif c['base_model_name'] == 'DO':\n",
    "                        samples = model.predict(dataset.X_test, 1)\n",
    "                                                \n",
    "                        results['model'] += ['DO']\n",
    "                        results['epoch'] += [dataset.curr_epoch]\n",
    "                        results['PLL']   += [pll(np.array([samples]), dataset.y_test, 1, model.tau)]\n",
    "                        results['RMSE']  += [rmse(samples, dataset.y_test)]\n",
    "                        results['CRPS']  += [crps(dataset.y_test, samples, model.tau**(-1))]\n",
    "                \n",
    "                \n",
    "                batch = dataset.next_batch(c['batch_size'])\n",
    "                if c['base_model_name'] in ['BN', 'MCBN']:\n",
    "                    model.run_train_step(batch)\n",
    "                elif c['base_model_name'] in ['DO', 'MCDO']:\n",
    "                    model.run_train_step(batch, keep_prob)\n",
    "    \n",
    "    if 'MC' in c['base_model_name']:\n",
    "        return DF(results), DF(final_predictions), DF(optimum_predictions)\n",
    "    return DF(results), None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_dataset_results(results_df, final_predictions_df, bm_opt_df, eval_path, dataset_name, base_model_name):\n",
    "    # Get dataset dir\n",
    "    dataset_path = os.path.join(eval_path, dataset_name)\n",
    "    make_path_if_missing(dataset_path)\n",
    "    \n",
    "    # Group by model and save\n",
    "    for model_name, model_df in results_df.groupby('model'):\n",
    "        model_df.reset_index(drop=True).to_csv(os.path.join(dataset_path, model_name + '.csv'))\n",
    "        \n",
    "    # Save final predictions dataframe\n",
    "    final_predictions_df.to_csv(os.path.join(dataset_path, base_model_name + ' final_predictions.csv'))\n",
    "    \n",
    "    # Save optimum predictions dataframe\n",
    "    bm_opt_df.to_csv(os.path.join(dataset_path, base_model_name + ' optimum_predictions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create parent evaluation dir\n",
    "eval_path = get_new_dir_in_parent_path(TEST_EVAL_PATH)\n",
    "\n",
    "# Save used setup, grid search and tau results\n",
    "dump_yaml(s, eval_path, 'eval_setup.yml')\n",
    "dump_yaml(g, eval_path, 'eval_grid_search_results.yml')\n",
    "dump_yaml(t, eval_path, 'tau_results.yml')\n",
    "\n",
    "all_results = None\n",
    "\n",
    "# Iterate over datasets to be evaluated\n",
    "for dataset_name in t.keys():\n",
    "    \n",
    "    logger.info(\"Dataset: \" + dataset_name)\n",
    "    \n",
    "    # Load dataset into memory\n",
    "    X_train, y_train, X_test, y_test = dl.load_uci_data_test(dataset_name)\n",
    "    feature_indices, target_indices = dl.load_uci_info(dataset_name)\n",
    "    \n",
    "    # Iterate over base optimization models (BN or DO)\n",
    "    train_and_evaluation_models = get_train_and_evaluation_models(s['models'])\n",
    "    \n",
    "    for bn_or_do_model, evaluation_models in train_and_evaluation_models.iteritems():\n",
    "                \n",
    "        # Get grid search parameters\n",
    "        bm_results_df = None\n",
    "        bm_final_pred_df = None\n",
    "        bm_opt_df = None\n",
    "        for base_model_name in evaluation_models:\n",
    "            \n",
    "            # Run multiple times based on n_testruns\n",
    "            for run_count in range(s['n_testruns']):\n",
    "            \n",
    "                # Set random generator seed for reproducible batch order\n",
    "                # Common for all base models for a certain dataset\n",
    "                np_seed = RANDOM_SEED_NP_FIRST_RUN + run_count\n",
    "                tf_seed = RANDOM_SEED_TF_FIRST_RUN + run_count\n",
    "                \n",
    "                np.random.seed(np_seed)\n",
    "                opt_dict = g[dataset_name][base_model_name]\n",
    "\n",
    "\n",
    "                logger.info(\"Model: {}, run: {} of {}\".format(base_model_name, run_count+1, s['n_testruns']))\n",
    "\n",
    "                # Get dataset configuration\n",
    "                c = {'base_model_name': base_model_name,\n",
    "                     'in_dim': len(feature_indices),\n",
    "                     'out_dim': len(target_indices),\n",
    "                     'k': s['k_specific'].get(dataset_name) or s['k'],\n",
    "                     'lambda': opt_dict['lambda'],\n",
    "                     'batch_size': opt_dict['batch_size'],\n",
    "                     'dropout': opt_dict.get('dropout'), # Can be None\n",
    "                     'taus': t[dataset_name],\n",
    "                     'n_epochs': g[dataset_name][base_model_name]['cv_epoch']\n",
    "                    }\n",
    "\n",
    "                df, df_fp, df_opt = evaluate_dataset(c, X_train, y_train, X_test, y_test, tf_seed)\n",
    "                \n",
    "                df['run_count'] = run_count+1\n",
    "                logger.info(df)\n",
    "                \n",
    "                bm_results_df = df if bm_results_df is None else bm_results_df.append(df, ignore_index=True)\n",
    "                \n",
    "                if df_fp is not None:\n",
    "                    df_fp['run_count'] = run_count+1\n",
    "                    bm_final_pred_df = df_fp if bm_final_pred_df is None else bm_final_pred_df.append(df_fp, ignore_index=True)\n",
    "                \n",
    "                if df_opt is not None:\n",
    "                    df_opt['run_count'] = run_count+1\n",
    "                    logger.info(df_opt)\n",
    "                    bm_opt_df = df_opt if bm_opt_df is None else bm_opt_df.append(df_opt, ignore_index=True)\n",
    "                \n",
    "        save_dataset_results(bm_results_df, bm_final_pred_df, bm_opt_df, eval_path, dataset_name, 'MC'+bn_or_do_model)\n",
    "\n",
    "        bm_results_df['dataset'] = dataset_name\n",
    "        all_results = bm_results_df if all_results is None else all_results.append(bm_results_df, ignore_index=True)\n",
    "\n",
    "    # Save all results\n",
    "    all_results.to_csv(os.path.join(eval_path, 'results.csv'))\n",
    "    \n",
    "logger.info(\"DONE STEP 6\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
