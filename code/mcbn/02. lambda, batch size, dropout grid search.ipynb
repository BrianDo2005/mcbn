{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pandas import DataFrame as DF\n",
    "\n",
    "import mcbn.data.dataset_loaders as dl\n",
    "from mcbn.data.dataset import Dataset\n",
    "\n",
    "from mcbn.models.model_bn import ModelBN\n",
    "from mcbn.models.model_do import ModelDO\n",
    "\n",
    "from mcbn.utils.metrics import rmse\n",
    "from mcbn.utils.helper import get_setup\n",
    "from mcbn.utils.helper import get_lambdas_range\n",
    "from mcbn.utils.helper import get_train_and_evaluation_models\n",
    "from mcbn.utils.helper import get_new_dir_in_parent_path\n",
    "from mcbn.utils.helper import make_path_if_missing\n",
    "from mcbn.utils.helper import dump_yaml\n",
    "from mcbn.utils.helper import get_logger\n",
    "\n",
    "from mcbn.environment.constants import HYPERPARAMS_EVAL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "\n",
    "logger.info(\"STEP 2: Get best hyperparameters\")\n",
    "\n",
    "RANDOM_SEED_NP = 1\n",
    "RANDOM_SEED_TF = 1\n",
    "\n",
    "s = get_setup()\n",
    "\n",
    "# Get lambdas range\n",
    "s['lambdas'] = get_lambdas_range(s['lambda_min'], s['lambda_max'])\n",
    "\n",
    "# Tau must be set, but has no relevance for this optimization\n",
    "TAU = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cv_rmses(c, test_path, folds_path, X, y):\n",
    "    \n",
    "    # Store a graph for each fold in a list\n",
    "    models = []\n",
    "    datasets = []\n",
    "\n",
    "    # Get and initialize model\n",
    "    with tf.Graph().as_default() as g:\n",
    "\n",
    "        with g.device('/cpu:0'):\n",
    "            \n",
    "            # Set random generator seed for reproducible models\n",
    "            tf.set_random_seed(RANDOM_SEED_TF)\n",
    "            \n",
    "            for fold in range(s['n_folds']):\n",
    "                \n",
    "                # Get dataset for fold\n",
    "                X_train, y_train, X_val, y_val = dl.load_fold(folds_path, fold, X, y)\n",
    "                dataset = Dataset(X_train, \n",
    "                                  y_train, \n",
    "                                  X_val, \n",
    "                                  y_val, \n",
    "                                  s['discard_leftovers'],\n",
    "                                  normalize_X=s['normalize_X'], \n",
    "                                  normalize_y=s['normalize_y'])\n",
    "\n",
    "                # Store dataset in datasets list\n",
    "                datasets.append(dataset)\n",
    "                \n",
    "                with tf.name_scope(\"model_{}\".format(fold)) as scope:\n",
    "                    \n",
    "                    # Get graph\n",
    "                    if 'BN' == c['base_model_name']:\n",
    "                        model = ModelBN(s['n_hidden'],\n",
    "                                        K=c['k'],\n",
    "                                        nonlinearity=s['nonlinearity'],\n",
    "                                        bn=True,\n",
    "                                        do=False,\n",
    "                                        tau=TAU,\n",
    "                                        dataset=dataset,\n",
    "                                        in_dim=c['in_dim'],\n",
    "                                        out_dim=c['out_dim'])\n",
    "                    elif 'DO' == c['base_model_name']:\n",
    "                        keep_prob = 1 - c['dropout']\n",
    "                        model = ModelDO(s['n_hidden'], \n",
    "                                        K=c['k'], \n",
    "                                        nonlinearity=s['nonlinearity'], \n",
    "                                        bn=False, \n",
    "                                        do=True,\n",
    "                                        tau=TAU, \n",
    "                                        dataset=dataset, \n",
    "                                        in_dim=c['in_dim'], \n",
    "                                        out_dim=c['out_dim'],\n",
    "                                        first_layer_do=True)\n",
    "                    model.initialize(l2_lambda=c['lambda'], learning_rate=s['learning_rate'])\n",
    "\n",
    "                    # Store graph in models list\n",
    "                    models.append(model)\n",
    "        \n",
    "        # Create savers to save models after training - one per fold\n",
    "        savers = [tf.train.Saver(\n",
    "                        var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='model_{}'.format(fold))\n",
    "                  ) for fold in range(s['n_folds'])]\n",
    "        \n",
    "        # Start session (regular session is default session in with statement)\n",
    "        with tf.Session(config=tf.ConfigProto(\n",
    "                allow_soft_placement=False,\n",
    "                log_device_placement=False,\n",
    "                inter_op_parallelism_threads=1,\n",
    "                intra_op_parallelism_threads=1)) as sess:\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Keep track of best average RMSE over all folds\n",
    "            best_results = {em: {'epoch': [0 for n in range(s['n_folds'])], \n",
    "                                 'RMSEs': [np.inf for n in range(s['n_folds'])]}\n",
    "                            for em in c['evaluation_models']}\n",
    "            unfavorable_evaluations = {em: 0 for em in c['evaluation_models']}\n",
    "            eval_interval = s['hyperparam_eval_interval']\n",
    "            \n",
    "            while any(unfavorable_evaluations[em] < s['patience'] for em in c['evaluation_models']):\n",
    "                \n",
    "                curr_results = {em: [] for em in c['evaluation_models']}\n",
    "                \n",
    "                # Iterate over all (base) model graphs and corresponding folds\n",
    "                for i, (model, dataset) in enumerate(zip(models, datasets)):\n",
    "                    \n",
    "                    start_epoch = dataset.curr_epoch\n",
    "                    \n",
    "                    # Train model for eval_interval iterations\n",
    "                    while not dataset.at_end_of_epoch(start_epoch + eval_interval, c['batch_size']):\n",
    "                        batch = dataset.next_batch(c['batch_size'])\n",
    "                        if 'BN' == c['base_model_name']:\n",
    "                            model.run_train_step(batch)\n",
    "                        elif 'DO' == c['base_model_name']:\n",
    "                            model.run_train_step(batch, keep_prob)\n",
    "                    \n",
    "                    # Check that we haven't exceeded global maximum epochs limit\n",
    "                    # If so, break while loop immediately\n",
    "                    if dataset.curr_epoch > s['global_max_epochs']:\n",
    "                        break\n",
    "\n",
    "                    # Evaluate fold validation RMSE and append to results\n",
    "                    for em in [em for em in c['evaluation_models'] if not unfavorable_evaluations[em] >= s['patience']]:\n",
    "                        if em == 'MCBN':\n",
    "                            yHat, _ = model.predict_mc(s['mc_samples'], dataset.X_test, c['batch_size'])\n",
    "                        elif em == 'BN':\n",
    "                            model.update_layer_statistics(dataset.X_train)\n",
    "                            yHat = model.predict(dataset.X_test)\n",
    "                        elif em == 'MCDO':\n",
    "                            yHat, _ = model.predict_mc(s['mc_samples'], dataset.X_test, keep_prob)\n",
    "                        elif em == 'DO':\n",
    "                            yHat = model.predict(dataset.X_test, 1)\n",
    "                        curr_results[em].append(rmse(yHat, dataset.y_test))\n",
    "\n",
    "                # RMSE at eval_interval end found for all folds\n",
    "                # For each evaluation model, check if an average RMSE improvement was made\n",
    "                for em in [em for em in c['evaluation_models'] if not unfavorable_evaluations[em] >= s['patience']]:\n",
    "                    \n",
    "                    if np.mean(curr_results[em]) <= np.mean(best_results[em]['RMSEs']):\n",
    "\n",
    "                        # Store the new best results\n",
    "                        best_results[em]['epoch'] = [dataset.curr_epoch for dataset in datasets]\n",
    "                        best_results[em]['RMSEs'] = curr_results[em]\n",
    "                        unfavorable_evaluations[em] = 0\n",
    "\n",
    "                        # Save improved models for all folds\n",
    "                        for fold in range(s['n_folds']):\n",
    "                            trained_model_dir = os.path.join(test_path, em, 'fold_{}'.format(fold))\n",
    "                            make_path_if_missing(trained_model_dir)\n",
    "                            trained_model_file_path = os.path.join(trained_model_dir, 'model')\n",
    "                            savers[fold].save(sess, trained_model_file_path)                    \n",
    "                    else:\n",
    "                        unfavorable_evaluations[em] += 1\n",
    "\n",
    "                    logger.info((\"{}: epochs: {:.2f}, Curr mean: {:.4f}, \" +\n",
    "                           \"best mean: {:.4f} at epoch {:.2f}. Breaks: {}\").format(\n",
    "                        em,\n",
    "                        np.mean([dataset.curr_epoch for dataset in datasets]),\n",
    "                        np.mean(curr_results[em]),\n",
    "                        np.mean(best_results[em]['RMSEs']),\n",
    "                        np.mean(best_results[em]['epoch']),\n",
    "                        unfavorable_evaluations[em]\n",
    "                    ))\n",
    "    \n",
    "    # Convert best results to DataFrame and return\n",
    "    test_results_df = None\n",
    "    for em, em_results in best_results.iteritems():\n",
    "        model_df = DF(em_results)\n",
    "        model_df['model'] = em\n",
    "        test_results_df = model_df if test_results_df is None else test_results_df.append(model_df, ignore_index=True)\n",
    "    return test_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_config_results(config, base_model_path, X, y, results_df):\n",
    "    \n",
    "    # Set random generator seed for consistent batches\n",
    "    np.random.seed(RANDOM_SEED_NP)\n",
    "    \n",
    "    # Each test is a learning parameter combination\n",
    "    test_count = len(os.listdir(base_model_path))\n",
    "    test_path = get_new_dir_in_parent_path(base_model_path, 'test_' + str(test_count))\n",
    "    \n",
    "    # Get and save results\n",
    "    folds_path = os.path.join(base_model_path, '..', 'fold_indices')\n",
    "    test_results_df = get_cv_rmses(config, test_path, folds_path, X, y)\n",
    "    file_path = os.path.join(test_path, 'cv_results.csv')\n",
    "    test_results_df.to_csv(file_path)\n",
    "    \n",
    "    # Save config\n",
    "    dump_yaml(config, test_path, 'config.yml')\n",
    "\n",
    "    # Save results in collection (dropout can be None)\n",
    "    test_results_df = test_results_df.groupby('model')['RMSEs', 'epoch'].mean().reset_index()\n",
    "    test_results_df = test_results_df.rename(columns={'RMSEs': 'cv_rmse', 'epoch': 'cv_epoch'})\n",
    "    for k in ['dataset_name', 'batch_size', 'lambda', 'dropout']:\n",
    "        test_results_df[k] = config.get(k)\n",
    "    \n",
    "    # Print and return results for this test\n",
    "    logger.info(test_results_df)\n",
    "    return test_results_df if results_df is None else results_df.append(test_results_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Results DataFrame\n",
    "results_df = None\n",
    "\n",
    "# Create parent evaluation dir\n",
    "eval_path = get_new_dir_in_parent_path(HYPERPARAMS_EVAL_PATH)\n",
    "\n",
    "# Save setup\n",
    "dump_yaml(s, eval_path, 'eval_setup.yml')\n",
    "\n",
    "for dataset_name in s['datasets']:\n",
    "    logger.info(\"Evaluating dataset \" + dataset_name)\n",
    "    \n",
    "    # Load dataset in memory\n",
    "    X, y = dl.load_uci_data_full(dataset_name)\n",
    "    \n",
    "    # Create eval dir for dataset\n",
    "    dataset_path = get_new_dir_in_parent_path(eval_path, dataset_name)\n",
    "    \n",
    "    # Set random generator seed for reproducible folds\n",
    "    np.random.seed(RANDOM_SEED_NP)\n",
    "    \n",
    "    # Generate folds for this dataset\n",
    "    folds_path = dl.create_folds(dataset_name, s['n_folds'], s['inverted_cv_fraction'], dataset_path)\n",
    "\n",
    "    # Get dataset configuration\n",
    "    feature_indices, target_indices = dl.load_uci_info(dataset_name)\n",
    "\n",
    "    # Iterate over base models ('BN' and/or 'DO')\n",
    "    train_and_evaluation_models = get_train_and_evaluation_models(s['models'])\n",
    "    for base_model in train_and_evaluation_models.keys():\n",
    "        logger.info(\"Evaluating base model \" + base_model)\n",
    "        \n",
    "        c = {'dataset_name': dataset_name,\n",
    "         'in_dim': len(feature_indices),\n",
    "         'out_dim': len(target_indices),\n",
    "         'k': s['k_specific'].get(dataset_name) or s['k'],\n",
    "         'base_model_name': base_model,\n",
    "         'evaluation_models': train_and_evaluation_models[base_model]\n",
    "        }\n",
    "        \n",
    "        # Create an eval dir for the base model\n",
    "        model_path = get_new_dir_in_parent_path(dataset_path, base_model)\n",
    "        \n",
    "        # Start evaluation for this base model\n",
    "        if base_model == 'BN':\n",
    "            \n",
    "            # Iterate over batch sizes\n",
    "            for bs in s['batch_sizes_specific'].get(dataset_name, s['batch_sizes']):\n",
    "                c['batch_size'] = bs\n",
    "            \n",
    "                # Iterate over L2 regularization lambdas\n",
    "                for l in s['lambdas']:\n",
    "                    c['lambda'] = l\n",
    "                    results_df = get_config_results(c, model_path, X, y, results_df)\n",
    "                    \n",
    "        # For MDCO, iterate over dropout probabilities\n",
    "        elif base_model == 'DO':\n",
    "            \n",
    "            # Batch size is fixed\n",
    "            c['batch_size'] = s['dropout_batch_size']\n",
    "            \n",
    "            # Iterate over dropout probabilities\n",
    "            for dropout in s['dropouts']:\n",
    "                c['dropout'] = dropout\n",
    "            \n",
    "                # Iterate over L2 regularization lambdas\n",
    "                for l in s['lambdas']:\n",
    "                    c['lambda'] = l\n",
    "                    results_df = get_config_results(c, model_path, X, y, results_df)\n",
    "    \n",
    "    # Save summary for dataset\n",
    "    dataset_results_path = os.path.join(dataset_path, 'dataset-results.csv')\n",
    "    dataset_df = results_df[results_df['dataset_name'] == dataset_name].reset_index(drop = True)\n",
    "    dataset_df.to_csv(dataset_results_path)\n",
    "\n",
    "# Save dataframe with all results\n",
    "all_results_path = os.path.join(eval_path, 'results.csv')\n",
    "results_df.to_csv(all_results_path)\n",
    "logger.info(\"DONE STEP 2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
