{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skopt import gbrt_minimize\n",
    "from pandas import DataFrame as DF\n",
    "\n",
    "import mcbn.data.dataset_loaders as dl\n",
    "from mcbn.data.dataset import Dataset\n",
    "\n",
    "from mcbn.models.model_bn import ModelBN\n",
    "from mcbn.models.model_do import ModelDO\n",
    "\n",
    "from mcbn.utils.metrics import pll\n",
    "from mcbn.utils.metrics import crps\n",
    "from mcbn.utils.helper import get_setup\n",
    "from mcbn.utils.helper import get_grid_search_results\n",
    "from mcbn.utils.helper import dump_yaml\n",
    "from mcbn.utils.helper import get_new_dir_in_parent_path\n",
    "from mcbn.utils.helper import make_path_if_missing\n",
    "from mcbn.utils.helper import get_logger\n",
    "from mcbn.environment.constants import TAU_EVAL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "\n",
    "logger.info(\"STEP 4: Running TAU optimization\")\n",
    "\n",
    "RANDOM_SEED_NP = 2\n",
    "RANDOM_SEED_TF = 1\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(c, X, y, tau):\n",
    "    tau = tau[0]\n",
    "    \n",
    "    avg_metric_vals = []\n",
    "    \n",
    "    # Get path of trained models for all folds\n",
    "    trained_models_path = os.path.join(os.getcwd(), c['trained_models_path'])\n",
    "    \n",
    "    # Get path of indices for all folds\n",
    "    folds_indices_path = os.path.join(trained_models_path, '..', '..', '..', 'fold_indices')\n",
    "    \n",
    "    # Iterate over folds\n",
    "    for fold_index, fold_model_dir in enumerate(sorted(os.listdir(trained_models_path))):\n",
    "        \n",
    "        # Get path of trained model for this fold\n",
    "        fold_model_path = os.path.join(trained_models_path, fold_model_dir)\n",
    "        \n",
    "        # Create a dataset from fold indices\n",
    "        X_train, y_train, X_val, y_val = dl.load_fold(folds_indices_path, fold_index, X, y)\n",
    "        dataset = Dataset(X_train, \n",
    "                          y_train, \n",
    "                          X_val, \n",
    "                          y_val, \n",
    "                          s['discard_leftovers'],\n",
    "                          normalize_X=s['normalize_X'], \n",
    "                          normalize_y=s['normalize_y'])\n",
    "            \n",
    "        # Load the saved model\n",
    "        with tf.Graph().as_default() as g:\n",
    "            \n",
    "            with g.device('/cpu:0'):\n",
    "                \n",
    "                # Set random generator seed for reproducible models\n",
    "                tf.set_random_seed(RANDOM_SEED_TF)\n",
    "                \n",
    "                with tf.name_scope(\"model_{}\".format(fold_index)) as scope:\n",
    "                    if c['base_model_name'] in ['BN', 'MCBN']:\n",
    "                        model = ModelBN(c['n_hidden'],\n",
    "                                        K=c['k'],\n",
    "                                        nonlinearity=c['nonlinearity'],\n",
    "                                        bn=True,\n",
    "                                        do=False,\n",
    "                                        tau=tau,\n",
    "                                        dataset=dataset,\n",
    "                                        in_dim=c['in_dim'],\n",
    "                                        out_dim=c['out_dim'])\n",
    "                    elif c['base_model_name'] in ['DO', 'MCDO']:\n",
    "                        keep_prob = 1 - c['dropout']\n",
    "                        model = ModelDO(c['n_hidden'], \n",
    "                                        K=c['k'], \n",
    "                                        nonlinearity=c['nonlinearity'], \n",
    "                                        bn=False, \n",
    "                                        do=True,\n",
    "                                        tau=tau, \n",
    "                                        dataset=dataset, \n",
    "                                        in_dim=c['in_dim'], \n",
    "                                        out_dim=c['out_dim'],\n",
    "                                        first_layer_do=True)\n",
    "\n",
    "                    model.initialize(l2_lambda=c['lambda'], learning_rate=c['learning_rate'])\n",
    "\n",
    "            #Start session (regular session is default session in with statement)\n",
    "            saver = tf.train.Saver()\n",
    "            with tf.Session(config=tf.ConfigProto(\n",
    "                    allow_soft_placement=False,\n",
    "                    log_device_placement=False,\n",
    "                    inter_op_parallelism_threads=1,\n",
    "                    intra_op_parallelism_threads=1)) as sess:\n",
    "                \n",
    "                saver.restore(sess, tf.train.latest_checkpoint(fold_model_path))\n",
    "\n",
    "                #Get CV PLL\n",
    "                if c['model_name'] == 'MCBN':\n",
    "                    samples = model.get_mc_samples(c['mc_samples'], dataset.X_test, c['batch_size'])\n",
    "                    mean, var = model.get_mc_moments(samples)\n",
    "                    if s['tau_opt_metric'] == 'PLL':\n",
    "                        avg_metric = pll(samples, dataset.y_test, c['mc_samples'], tau)\n",
    "                    else:\n",
    "                        avg_metric = crps(dataset.y_test, mean, var)\n",
    "                \n",
    "                elif c['model_name'] == 'MCBN const':\n",
    "                    samples = model.get_mc_samples(c['mc_samples'], dataset.X_test, c['batch_size'])\n",
    "                    mean, var = model.get_mc_moments(samples)\n",
    "                    if s['tau_opt_metric'] == 'PLL':\n",
    "                        avg_metric = pll(np.array([mean]), dataset.y_test, 1, tau)\n",
    "                    else:\n",
    "                        avg_metric = crps(dataset.y_test, mean, tau**(-1))\n",
    "                    \n",
    "                elif c['model_name'] == 'BN':\n",
    "                    model.update_layer_statistics(dataset.X_test)\n",
    "                    samples = model.predict(dataset.X_test)\n",
    "                    if s['tau_opt_metric'] == 'PLL':\n",
    "                        avg_metric = pll(np.array([samples]), dataset.y_test, 1, tau)\n",
    "                    else:\n",
    "                        avg_metric = crps(dataset.y_test, samples, tau**(-1))\n",
    "                    \n",
    "                    \n",
    "                elif c['model_name'] == 'MCDO':\n",
    "                    samples = model.get_mc_samples(c['mc_samples'], dataset.X_test, keep_prob)\n",
    "                    mean, var = model.get_mc_moments(samples)\n",
    "                    if s['tau_opt_metric'] == 'PLL':\n",
    "                        avg_metric = pll(samples, dataset.y_test, c['mc_samples'], tau)\n",
    "                    else:\n",
    "                        avg_metric = crps(dataset.y_test, mean, var)\n",
    "                    \n",
    "                elif c['model_name'] == 'MCDO const':\n",
    "                    samples = model.get_mc_samples(c['mc_samples'], dataset.X_test, keep_prob)\n",
    "                    mean, var = model.get_mc_moments(samples)\n",
    "                    if s['tau_opt_metric'] == 'PLL':\n",
    "                        avg_metric = pll(np.array([mean]), dataset.y_test, 1, tau)\n",
    "                    else:\n",
    "                        avg_metric = crps(dataset.y_test, mean, tau**(-1))\n",
    "                \n",
    "                elif c['model_name'] == 'DO':\n",
    "                    samples = model.predict(dataset.X_test, 1)\n",
    "                    if s['tau_opt_metric'] == 'PLL':\n",
    "                        avg_metric = pll(np.array([samples]), dataset.y_test, 1, tau)\n",
    "                    else:\n",
    "                        avg_metric = crps(dataset.y_test, samples, tau**(-1))\n",
    "\n",
    "        avg_metric_vals += [avg_metric]\n",
    "    \n",
    "    ALL_TAUS.append(tau)\n",
    "    ALL_PLLS.append(np.mean(avg_metric_vals))\n",
    "    \n",
    "    logger.info(\"tau {:.10f}: Avg {} {:.4f}\".format(tau, s['tau_opt_metric'], np.mean(avg_metric_vals)))\n",
    "    \n",
    "    #Minimization objective: avg CRPS or avg. negative PLLs\n",
    "    return (-1 if s['tau_opt_metric'] == 'PLL' else 1) * np.mean(avg_metric_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_results(tau_opt, eval_path, c):\n",
    "    \n",
    "    # Get dataset dir\n",
    "    dataset_path = os.path.join(eval_path, c['dataset_name'])\n",
    "    make_path_if_missing(dataset_path)\n",
    "    \n",
    "    #Save csv for model\n",
    "    df = DF({'tau':ALL_TAUS, 'pll':ALL_PLLS})\n",
    "    df.to_csv(os.path.join(dataset_path, c['model_name'] + '.csv'))\n",
    "    \n",
    "    #Save plot for model\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    plt.scatter(ALL_TAUS, ALL_PLLS)\n",
    "    plt.xlabel('TAU')\n",
    "    plt.ylabel('PLL')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(dataset_path, c['model_name'] + '.png'))\n",
    "    \n",
    "    #Save best result for model\n",
    "    np.savetxt(os.path.join(dataset_path, c['model_name'] + '_opt.txt'), [tau_opt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = get_setup()\n",
    "g = get_grid_search_results()\n",
    "\n",
    "# Create parent evaluation dir\n",
    "eval_path = get_new_dir_in_parent_path(TAU_EVAL_PATH)\n",
    "\n",
    "# Save used setup and grid search results\n",
    "dump_yaml(s, eval_path, 'eval_setup.yml')\n",
    "dump_yaml(g, eval_path, 'eval_grid_search_results.yml')\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over each dataset\n",
    "for dataset_name, models_dict in g.iteritems():\n",
    "    \n",
    "    logger.info(\"Dataset: \" + dataset_name)\n",
    "    \n",
    "    # Load dataset in memory\n",
    "    X, y = dl.load_uci_data_full(dataset_name)\n",
    "    feature_indices, target_indices = dl.load_uci_info(dataset_name)\n",
    "    \n",
    "    # Get min and max of optimization range for tau\n",
    "    ranges = s['tau_range'][dataset_name]\n",
    "    tau_min = ranges[0]\n",
    "    tau_max = ranges[1]\n",
    "    \n",
    "    # Add 'const' variants of MC models if present\n",
    "    optimized_MC_models = [m for m in models_dict.keys() if 'MC' in m]\n",
    "    for mc_model in optimized_MC_models:\n",
    "        models_dict[mc_model + ' const'] = models_dict[mc_model]\n",
    "        \n",
    "    # Iterate over each model's optimized learning parameters for this dataset\n",
    "    for model_name, opt_dict in models_dict.iteritems():\n",
    "        \n",
    "        # Set random generator seed for identical batch picks for standard and const variant\n",
    "        np.random.seed(RANDOM_SEED_NP)\n",
    "        \n",
    "        logger.info(\"Model: \" + model_name)\n",
    "            \n",
    "        # Get dataset configuration\n",
    "        c = {'dataset_name': dataset_name,\n",
    "             'base_model_name': model_name.replace(' const', ''),\n",
    "             'model_name': model_name,\n",
    "             'in_dim': len(feature_indices),\n",
    "             'out_dim': len(target_indices),\n",
    "             'n_hidden': s['n_hidden'],\n",
    "             'k': s['k_specific'].get(dataset_name) or s['k'],\n",
    "             'nonlinearity': s['nonlinearity'],\n",
    "             'lambda': opt_dict['lambda'],\n",
    "             'batch_size': opt_dict['batch_size'],\n",
    "             'trained_models_path': opt_dict['path'],\n",
    "             'dropout': opt_dict.get('dropout'), # Can be None\n",
    "             'learning_rate': s['learning_rate'],\n",
    "             'mc_samples': s['mc_samples']\n",
    "             }\n",
    "\n",
    "        # Save incremental results during optimization\n",
    "        ALL_TAUS = []\n",
    "        ALL_PLLS = []\n",
    "        \n",
    "        # Make optimization run take extra arguments\n",
    "        optimize_fun = partial(run, c, X, y)      \n",
    "        \n",
    "        tau_opt = gbrt_minimize(optimize_fun, \n",
    "                                [(tau_min, tau_max)], \n",
    "                                n_random_starts=s['tau_opt_n_random_calls'], \n",
    "                                n_calls=s['tau_opt_n_total_calls'])\n",
    "        save_results(tau_opt.x[0], eval_path, c)\n",
    "        \n",
    "logger.info(\"DONE STEP 4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
